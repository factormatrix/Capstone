import os
import io
import numpy as np
import pandas as pd
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score,
    mean_squared_error, r2_score
)
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

import shap
import matplotlib.pyplot as plt

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False


# -----------------------
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# -----------------------
def read_csv_auto(file_obj):
    """
    ì—…ë¡œë“œëœ CSV íŒŒì¼ì„ ì¸ì½”ë”©ì„ ë°”ê¿” ê°€ë©° ì½ëŠ” í•¨ìˆ˜.
    UTF-8ì´ ì•ˆ ë˜ë©´ cp949, ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ISO-8859-1 ì‹œë„.
    """
    import pandas as pd

    # 1) UTF-8 ë¨¼ì € ì‹œë„
    try:
        file_obj.seek(0)
        return pd.read_csv(file_obj, encoding="utf-8")
    except UnicodeDecodeError:
        pass

    # 2) cp949 (í•œê¸€ ìœˆë„ìš° ê¸°ë³¸)
    try:
        file_obj.seek(0)
        return pd.read_csv(file_obj, encoding="cp949")
    except UnicodeDecodeError:
        pass

    # 3) ë§ˆì§€ë§‰ìœ¼ë¡œ ISO-8859-1 ê°™ì€ ë²”ìš© ì¸ì½”ë”©
    file_obj.seek(0)
    return pd.read_csv(file_obj, encoding="iso-8859-1", errors="replace")


def detect_cost_columns(df):
    """
    Q-COST ê´€ë ¨ ì»¬ëŸ¼ ìë™ íƒì§€ (ì¶”ì²œìš©)
    - ì˜ˆë°©ë¹„ìš©(prevention)
    - í‰ê°€/ê²€ì‚¬ë¹„ìš©(appraisal)
    - ë‚´ë¶€ ì‹¤íŒ¨(internal failure)
    - ì™¸ë¶€ ì‹¤íŒ¨(external failure)
    """
    cols = df.columns

    def match_keywords(keywords):
        return [
            c for c in cols
            if any(k.lower() in str(c).lower() for k in keywords)
        ]

    prevention_cols = match_keywords(["ì˜ˆë°©", "prevention", "prevention_cost", "prevention cost"])
    appraisal_cols = match_keywords(["í‰ê°€", "ê²€ì‚¬", "inspection", "appraisal"])
    internal_failure_cols = match_keywords(["ë‚´ë¶€", "internal_failure", "internal failure"])
    external_failure_cols = match_keywords(["ì™¸ë¶€", "external_failure", "external failure"])

    return {
        "prevention": prevention_cols,
        "appraisal": appraisal_cols,
        "internal_failure": internal_failure_cols,
        "external_failure": external_failure_cols,
    }


def detect_target_column(df):
    """
    ì„±ê³µ/ì‹¤íŒ¨, ì–‘í’ˆ/ë¶ˆëŸ‰ ë“± íƒ€ê¹ƒ ì»¬ëŸ¼ ìë™ íƒì§€
    - ê°’ì´ 2~3ê°œ ì •ë„ì¸ ì»¬ëŸ¼ + ì´ë¦„ íŒ¨í„´ ê¸°ë°˜
    """
    candidates = []
    for col in df.columns:
        unique_vals = df[col].dropna().unique()
        if 1 < len(unique_vals) <= 3:
            candidates.append(col)

    # ì´ë¦„ íŒ¨í„´ ìš°ì„  íƒìƒ‰
    preferred_patterns = [
        "ì„±ê³µ", "ì‹¤íŒ¨", "í•©ê²©", "ë¶ˆí•©ê²©", "ë¶ˆëŸ‰", "ì–‘í’ˆ",
        "pass_fail", "passfail", "pass", "fail",
        "target", "label", "y"
    ]
    for col in candidates:
        name = str(col).lower()
        if any(p.lower() in name for p in preferred_patterns):
            return col

    # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í›„ë³´ ë˜ëŠ” None
    return candidates[0] if candidates else None


def binarize_target(series):
    """
    ì„±ê³µ/ì‹¤íŒ¨ í…ìŠ¤íŠ¸ë¥¼ 0/1ë¡œ ë§µí•‘
    """
    s = series.copy()

    mapping = {
        "ì„±ê³µ": 1, "success": 1, "pass": 1, "í•©ê²©": 1,
        "ì‹¤íŒ¨": 0, "fail": 0, "ë¶ˆí•©ê²©": 0, "ë¶ˆëŸ‰": 0, "ì–‘í’ˆ": 1
    }

    def _map(v):
        if pd.isna(v):
            return np.nan
        v_str = str(v).strip().lower()
        if v_str in mapping:
            return mapping[v_str]
        # ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ
        try:
            return float(v)
        except Exception:
            return np.nan

    s = s.map(_map)

    # 0/1ë§Œ ë‚¨ê¸°ê¸°
    unique_vals = [u for u in s.dropna().unique()]
    # ì´ë¯¸ 0/1ì´ë©´ ê·¸ëŒ€ë¡œ
    if set(unique_vals).issubset({0, 1}):
        return s

    # 2ê°œ ê°’ì´ë©´ ì‘ì€ ê°’ 0, í° ê°’ 1ë¡œ ê°•ì œ ë§¤í•‘
    if len(unique_vals) == 2:
        lo, hi = sorted(unique_vals)
        return s.map(lambda x: 0 if x == lo else (1 if x == hi else np.nan))

    # ê·¸ ì™¸ì—ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë‚˜ì¤‘ì— ì—°ì†í˜•ìœ¼ë¡œ ì·¨ê¸‰)
    return s


def train_models_classification(X, y):
    results = {}

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 1) ë¡œì§€ìŠ¤í‹± íšŒê·€
    logreg = LogisticRegression(max_iter=500)
    logreg.fit(X_train, y_train)
    y_pred = logreg.predict(X_test)
    y_prob = logreg.predict_proba(X_test)[:, 1]

    results["Logistic Regression"] = {
        "model": logreg,
        "accuracy": accuracy_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
        "auc": roc_auc_score(y_test, y_prob),
    }

    # 2) ëœë¤í¬ë ˆìŠ¤íŠ¸
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    y_prob = rf.predict_proba(X_test)[:, 1]

    results["RandomForest"] = {
        "model": rf,
        "accuracy": accuracy_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
        "auc": roc_auc_score(y_test, y_prob),
    }

    # 3) XGBoost (ìˆìœ¼ë©´)
    if XGBOOST_AVAILABLE:
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=4,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric="logloss",
            random_state=42,
            n_jobs=-1,
        )
        xgb_model.fit(X_train, y_train)
        y_pred = xgb_model.predict(X_test)
        y_prob = xgb_model.predict_proba(X_test)[:, 1]

        results["XGBoost"] = {
            "model": xgb_model,
            "accuracy": accuracy_score(y_test, y_pred),
            "f1": f1_score(y_test, y_pred),
            "auc": roc_auc_score(y_test, y_prob),
        }

    return results, (X_train, X_test, y_train, y_test)


def train_models_regression(X, y):
    """
    íšŒê·€ ëª¨ë¸ í•™ìŠµ (squared=False ì•ˆ ì“°ê³ , RMSE ì§ì ‘ ê³„ì‚°)
    """
    results = {}

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 1) ì„ í˜• íšŒê·€
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    results["Linear Regression"] = {
        "model": lr,
        "rmse": rmse,
        "r2": r2_score(y_test, y_pred),
    }

    # 2) ëœë¤í¬ë ˆìŠ¤íŠ¸
    rf = RandomForestRegressor(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    results["RandomForest"] = {
        "model": rf,
        "rmse": rmse,
        "r2": r2_score(y_test, y_pred),
    }

    # 3) XGBoost (ìˆìœ¼ë©´)
    if XGBOOST_AVAILABLE:
        xgb_model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=4,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
        )
        xgb_model.fit(X_train, y_train)
        y_pred = xgb_model.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)

        results["XGBoost"] = {
            "model": xgb_model,
            "rmse": rmse,
            "r2": r2_score(y_test, y_pred),
        }

    return results, (X_train, X_test, y_train, y_test)

def remove_failure_related_features(X, failure_cols):
    """
    ì‹¤íŒ¨ë¹„ìš© ê´€ë ¨ ì»¬ëŸ¼(ë‚´ë¶€/ì™¸ë¶€/í†µí•©)ì„ X(ì…ë ¥ íŠ¹ì§•)ì—ì„œ ì œê±°í•˜ì—¬
    ëª¨ë¸ ëˆ„ì¶œ(leakage)ì„ ë°©ì§€í•œë‹¤.
    """
    failure_cols = [c for c in failure_cols if c in X.columns]
    return X.drop(columns=failure_cols, errors="ignore")


def plot_feature_importance(model, feature_names, title="Feature Importance"):
    # (ê·¸ë˜í”„ìš© í•¨ìˆ˜ â€“ ì§€ê¸ˆì€ í˜¸ì¶œí•˜ì§€ ì•Šì§€ë§Œ ë‚¨ê²¨ë‘ )
    importances = getattr(model, "feature_importances_", None)
    if importances is None:
        st.info("ì´ ëª¨ë¸ì€ feature_importances_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    idx = np.argsort(importances)[::-1]
    sorted_names = np.array(feature_names)[idx]
    sorted_vals = importances[idx]

    plt.figure(figsize=(6, 4))
    plt.bar(range(len(sorted_vals)), sorted_vals)
    plt.xticks(range(len(sorted_vals)), sorted_names, rotation=90)
    plt.title(title)
    plt.tight_layout()
    st.pyplot(plt.gcf())
    plt.close()


def plot_shap_summary_tree(model, X_train, feature_names, title="SHAP Summary"):
    # (ê·¸ë˜í”„ìš© í•¨ìˆ˜ â€“ ì§€ê¸ˆì€ í˜¸ì¶œí•˜ì§€ ì•Šì§€ë§Œ ë‚¨ê²¨ë‘ )
    st.write(f"### {title}")
    sample = X_train
    if X_train.shape[0] > 500:
        sample = X_train.sample(500, random_state=42)

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(sample)

    plt.figure()
    try:
        # ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš° shap_values[1] ì‚¬ìš©
        if isinstance(shap_values, list) and len(shap_values) == 2:
            shap.summary_plot(shap_values[1], sample, feature_names=feature_names, show=False)
        else:
            shap.summary_plot(shap_values, sample, feature_names=feature_names, show=False)
        st.pyplot(plt.gcf())
    finally:
        plt.close()


def build_scenario_result(df, cost_cols, failure_col_name=None):
    """
    ì˜ˆë°©/í‰ê°€ ë¹„ìš©ì„ ë³€ê²½í–ˆì„ ë•Œ ì‹¤íŒ¨ë¹„ìš© ë³€í™” ì‹œë®¬ë ˆì´ì…˜
    - ì‹¤íŒ¨ë¹„ìš©: ì‚¬ìš©ìê°€ ì§€ì •í•œ í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ ë˜ëŠ” ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš© í•©ê³„
    - íƒ€ê¹ƒ: ì‹¤íŒ¨ë¹„ìš© (íšŒê·€)
    """
    # ì˜ˆë°©/í‰ê°€ ë¹„ìš© ì»¬ëŸ¼
    prevention_cols = cost_cols.get("prevention", [])
    appraisal_cols = cost_cols.get("appraisal", [])
    internal_failure_cols = cost_cols.get("internal_failure", [])
    external_failure_cols = cost_cols.get("external_failure", [])

    # ìˆ«ìí˜• ë°ì´í„°ë§Œ ì‚¬ìš©
    df_num = df.select_dtypes(include=[np.number]).copy()

    # 1) ìš°ì„  ì‚¬ìš©ìê°€ ì§€ì •í•œ í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì„ ì‚¬ìš©
    failure_cols = []
    if failure_col_name is not None and failure_col_name in df_num.columns:
        failure_cols = [failure_col_name]
    else:
        # 2) í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš© í•©ì‚°
        failure_cols = [c for c in (internal_failure_cols + external_failure_cols) if c in df_num.columns]

    if not failure_cols:
        return None, "ì‹¤íŒ¨ë¹„ìš©(íƒ€ê¹ƒ) ìˆ˜ì¹˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì„ ì§€ì •í•˜ê±°ë‚˜ ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”."

    # ì‹¤íŒ¨ë¹„ìš© íƒ€ê¹ƒ ìƒì„±
    df_num["failure_cost"] = df_num[failure_cols].sum(axis=1)

    # íŠ¹ì§• ë³€ìˆ˜ë¡œ ì‚¬ìš©í•  í›„ë³´ (ì˜ˆë°© + í‰ê°€ ë¹„ìš© í¬í•¨)
    feature_cols = list(set(
        [c for c in prevention_cols + appraisal_cols if c in df_num.columns]
    ))

    # ì—†ìœ¼ë©´ ì „ì²´ ìˆ«ì ì»¬ëŸ¼ì—ì„œ failure_cost ì œì™¸í•˜ê³  ì‚¬ìš©
    if not feature_cols:
        feature_cols = [c for c in df_num.columns if c != "failure_cost"]

    # ê²°ì¸¡ì¹˜ ì œê±°
    data = df_num[feature_cols + ["failure_cost"]].dropna()
    if data.shape[0] < 20:
        return None, "ì‹œë®¬ë ˆì´ì…˜ì„ í•˜ê¸°ì—ëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤."

    X = data[feature_cols]
    y = data["failure_cost"]

    failure_related_cols = internal_failure_cols + external_failure_cols
    if failure_col_name is not None:
        failure_related_cols.append(failure_col_name)

    X = remove_failure_related_features(X, failure_related_cols)

    results, (X_train, X_test, y_train, y_test) = train_models_regression(X, y)

    # ê°€ì¥ ë‹¨ìˆœí•œ ëœë¤í¬ë ˆìŠ¤íŠ¸ ì‚¬ìš©
    if "RandomForest" in results:
        model = results["RandomForest"]["model"]
    else:
        model = list(results.values())[0]["model"]

    # baseline: ì „ì²´ í‰ê· ê°’ í•œ ì ì—ì„œ ì˜ˆì¸¡
    base_point = X.mean(axis=0).to_frame().T

    def predict_with_factor(prevention_factor, appraisal_factor):
        x_new = base_point.copy()
        for c in prevention_cols:
            if c in x_new.columns:
                x_new[c] = x_new[c] * (1 + prevention_factor)
        for c in appraisal_cols:
            if c in x_new.columns:
                x_new[c] = x_new[c] * (1 + appraisal_factor)
        return float(model.predict(x_new)[0])

    baseline_cost = predict_with_factor(0.0, 0.0)

    return {
        "model": model,
        "feature_cols": feature_cols,
        "prevention_cols": prevention_cols,
        "appraisal_cols": appraisal_cols,
        "baseline_cost": baseline_cost,
        "predict_func": predict_with_factor,
        "metrics": results
    }, None


# -----------------------
# Google Generative AI ì±—ë´‡
# -----------------------
def generate_ai_response(user_message, api_key, analysis_summary=""):
    import google.generativeai as genai

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash")

    system_prompt = f"""
ë‹¹ì‹ ì€ ì¤‘ì†Œê¸°ì—…ì˜ Q-COST(ì˜ˆë°©ë¹„ìš©, í‰ê°€ë¹„ìš©, ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš©) ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
íšŒê·€ë¶„ì„, ëœë¤í¬ë ˆìŠ¤íŠ¸, XGBoost, SHAP ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í’ˆì§ˆ ë¹„ìš© êµ¬ì¡°ì™€ ì˜ì‚¬ê²°ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ì•„ë˜ëŠ” í˜„ì¬ ë°ì´í„° ë¶„ì„ ìš”ì•½ì…ë‹ˆë‹¤:

{analysis_summary}

ì´ ìš”ì•½ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´:
- í•œêµ­ì–´ë¡œ,
- ìµœëŒ€í•œ ì‰½ê²Œ,
- ìˆ«ìì™€ ì§ê´€ì  í‘œí˜„ì„ í•¨ê»˜ ì‚¬ìš©í•´ì„œ
ì„¤ëª…í•´ ì£¼ì„¸ìš”.
"""

    full_prompt = system_prompt + "\n\nì‚¬ìš©ì ì§ˆë¬¸:\n" + user_message

    response = model.generate_content(full_prompt)
    return response.text


def get_feature_importance_df(model, feature_names):
    """
    íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸(RandomForest, XGBoost ë“±)ì˜ feature_importances_ë¥¼
    ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ DataFrameìœ¼ë¡œ ë°˜í™˜
    """
    importances = getattr(model, "feature_importances_", None)
    if importances is None:
        return None

    imp_df = pd.DataFrame({
        "feature": feature_names,
        "importance": importances
    })
    imp_df = imp_df.sort_values("importance", ascending=False).reset_index(drop=True)
    return imp_df


def get_shap_importance_df(model, X_train, feature_names):
    """
    SHAP ê°’ì„ ì´ìš©í•´ ê° ë³€ìˆ˜ì˜ í‰ê·  |SHAP| (ì ˆëŒ€ê°’) ì¤‘ìš”ë„ë¥¼
    ìˆ«ì í…Œì´ë¸”ë¡œ ë°˜í™˜.
    - shap_valuesê°€ listì´ë“ , 3ì°¨ì› ì´ìƒì´ë“  ìµœëŒ€í•œ 2D (n_samples, n_features)ë¡œ ì •ë¦¬í•´ì„œ ì‚¬ìš©.
    """
    sample = X_train

    # ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§
    if X_train.shape[0] > 500:
        sample = X_train.sample(500, random_state=42)

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(sample)

    # 1) ì´ì§„/ë‹¤ì¤‘ ë¶„ë¥˜: shap_valuesê°€ [class0, class1, ...] ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
    sv = shap_values
    if isinstance(sv, list):
        # ë³´í†µ "ì–‘ì„±" í´ë˜ìŠ¤(1ë²ˆ)ë¥¼ ë§ì´ ë³´ì§€ë§Œ,
        # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í´ë˜ìŠ¤ ì‚¬ìš©
        if len(sv) > 1:
            sv = sv[1]
        else:
            sv = sv[0]

    # 2) numpy arrayë¡œ í†µì¼
    sv = np.array(sv)

    # sv ì°¨ì› ì •ë¦¬:
    # ë³´í†µ (n_samples, n_features)ì¸ë°, ê°„í˜¹ (1, n_samples, n_features) ë“±ìœ¼ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ
    if sv.ndim > 2:
        # ë§ˆì§€ë§‰ ì¶•ì€ featureë¡œ ë³´ê³ , ë‚˜ë¨¸ì§€ ì¶•ì€ ì „ë¶€ sample ì°¨ì›ìœ¼ë¡œ í¼ì¹¨
        n_features = sv.shape[-1]
        sv = sv.reshape(-1, n_features)

    # ì´ì œ svëŠ” (n_samples, n_features) í˜•íƒœë¼ê³  ê°€ì •
    mean_abs_shap = np.mean(np.abs(sv), axis=0)  # -> (n_features,)

    # í˜¹ì‹œë¼ë„ ë‚¨ì€ ì°¨ì›ì´ ìˆìœ¼ë©´ í‰íƒ„í™”
    mean_abs_shap = np.array(mean_abs_shap).ravel()

    # feature_namesë„ ë¦¬ìŠ¤íŠ¸ë¡œ ìºìŠ¤íŒ…
    feature_names = list(feature_names)

    # ê¸¸ì´ê°€ ì•ˆ ë§ìœ¼ë©´ ìµœì†Œ ê¸¸ì´ì— ë§ì¶° ìë¥´ê¸° (ë°©ì–´ ì½”ë“œ)
    if len(feature_names) != len(mean_abs_shap):
        n = min(len(feature_names), len(mean_abs_shap))
        feature_names = feature_names[:n]
        mean_abs_shap = mean_abs_shap[:n]

    shap_df = pd.DataFrame({
        "feature": feature_names,
        "mean_abs_shap": mean_abs_shap
    })
    shap_df = shap_df.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)

    return shap_df



# -----------------------
# Streamlit ì•± ì‹œì‘
# -----------------------
st.set_page_config(page_title="Q-COST AI Chat", layout="wide")

st.markdown(
    "<h1 style='text-align: center;'>Q-COST AI Chat</h1>",
    unsafe_allow_html=True
)

# ì‚¬ì´ë“œë°”: Google API Key & ì˜µì…˜
with st.sidebar:
    st.header("ğŸ”‘ ì„¤ì •")
    google_api_key = st.text_input(
        "Google API KEY ì…ë ¥ (Gemini ì‚¬ìš©)",
        type="password",
        help="Google Generative AI(Gemini) API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    )
    if google_api_key:
        st.success("API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âœ…")
    else:
        st.info("ì±—ë´‡ ê¸°ëŠ¥ì„ ì“°ë ¤ë©´ Google API KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.", icon="â„¹ï¸")

    st.markdown("---")
    st.markdown("**íŒŒì¼ ì—…ë¡œë“œ í›„ ë¶„ì„ ì¹¼ëŸ¼ì„ ì§€ì •í•´ì£¼ì„¸ìš”**")
    st.markdown("**ì´í›„ ìë™ìœ¼ë¡œ**")
    st.markdown("**íšŒê·€, ëœë¤í¬ë ˆìŠ¤íŠ¸, XGBoost, SHAP ì¤‘ìš”ë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.**")
    st.markdown("**ì˜ˆë°©/í‰ê°€ë¹„ìš© ì‹œë‚˜ë¦¬ì˜¤ë„ ì ìš©í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**")

tab_analysis, tab_chat = st.tabs(["ìë™ Q-COST ë¶„ì„", "Q-COST AI ëŒ€í™”"])

# ì„¸ì…˜ ìƒíƒœì— ë¶„ì„ ìš”ì•½ ì €ì¥ (ì±—ë´‡ì—ê²Œ ë„˜ê¸°ê¸°ìš©)
if "analysis_summary" not in st.session_state:
    st.session_state["analysis_summary"] = ""

with tab_analysis:
    st.subheader("ë°ì´í„° ì—…ë¡œë“œ")

    uploaded_file = st.file_uploader(
        "CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        type=["csv", "xlsx", "xls"]
    )

    if uploaded_file is not None:
        # íŒŒì¼ ì½ê¸°
        if uploaded_file.name.endswith(".csv"):
            df = read_csv_auto(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)


        st.write("#### ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df.head())

        # -------------------------
        # Q-COST ì»¬ëŸ¼ ìë™ íƒì§€ + ì§ì ‘ ì§€ì •
        # -------------------------
        st.write("### Q-COST ë¹„ìš© ì»¬ëŸ¼ ì§€ì • (ì˜ˆë°©/í‰ê°€)")
        auto_cost_cols = detect_cost_columns(df)
        st.caption("ìë™ìœ¼ë¡œ ì°¾ì•„ë³¸ ê²°ê³¼ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë„£ì–´ë‘ì—ˆì–´ìš”. í•„ìš”í•˜ë©´ ë“œë¡­ë‹¤ìš´ì—ì„œ ì§ì ‘ ë°”ê¿”ì£¼ì„¸ìš”.")

        all_columns = list(df.columns)

        col1, col2 = st.columns(2)
        with col1:
            prevention_selected = st.multiselect(
                "ì˜ˆë°©ë¹„ìš© ì»¬ëŸ¼ ì„ íƒ",
                options=all_columns,
                default=auto_cost_cols["prevention"],
                help="ì˜ˆë°© í™œë™, êµìœ¡, ì„¤ë¹„ ê°œì„  ë“±ì— ì“°ì´ëŠ” ë¹„ìš© ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”."
            )
        with col2:
            appraisal_selected = st.multiselect(
                "í‰ê°€/ê²€ì‚¬ë¹„ìš© ì»¬ëŸ¼ ì„ íƒ",
                options=all_columns,
                default=auto_cost_cols["appraisal"],
                help="ê²€ì‚¬, ì‹œí—˜, í’ˆì§ˆì ê²€ì— ë“¤ì–´ê°€ëŠ” ë¹„ìš© ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”."
            )

        # ì‹¤íŒ¨ë¹„ìš© íƒ€ê¹ƒ ì„¤ì •: í†µí•© ì‹¤íŒ¨ë¹„ìš© ë˜ëŠ” ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš© í•©ì‚°
        st.write("### ì‹¤íŒ¨ë¹„ìš©(íƒ€ê¹ƒ) ì»¬ëŸ¼ ì„¤ì •")
        st.caption("ë°ì´í„°ì— ì´ë¯¸ 'ì‹¤íŒ¨ë¹„ìš©'ì´ í•œ ì»¬ëŸ¼ìœ¼ë¡œ ìˆìœ¼ë©´ ì•„ë˜ì—ì„œ ë°”ë¡œ ì„ íƒí•˜ê³ , ë‚´ë¶€/ì™¸ë¶€ê°€ ë‚˜ë‰˜ì–´ ìˆìœ¼ë©´ ê°ê°ì˜ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”.")

        numeric_cols = df.select_dtypes(include=[np.number]).columns

        failure_single_col = st.selectbox(
            "í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ (ìˆëŠ” ê²½ìš°)",
            options=["(ì—†ìŒ)"] + list(numeric_cols),
            index=0
        )

        col3, col4 = st.columns(2)
        with col3:
            failure_internal_cols = st.multiselect(
                "ë‚´ë¶€ ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ ì„ íƒ",
                options=all_columns,
                default=auto_cost_cols["internal_failure"],
                help="ê³µì • ì•ˆì—ì„œ ë°œìƒí•˜ëŠ” ë¶ˆëŸ‰, ì¬ì‘ì—…, ìŠ¤í¬ë© ë¹„ìš© ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”."
            )
        with col4:
            failure_external_cols = st.multiselect(
                "ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ ì„ íƒ",
                options=all_columns,
                default=auto_cost_cols["external_failure"],
                help="ê³ ê° í´ë ˆì„, A/S, ë¦¬ì½œ ë“± ì™¸ë¶€ì—ì„œ ë°œìƒí•˜ëŠ” ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”."
            )

        # ì´í›„ ì½”ë“œì—ì„œ ì‚¬ìš©í•  ê³µì‹ cost_cols (ì‚¬ìš©ì ì„ íƒ ê¸°ì¤€)
        cost_cols = {
            "prevention": prevention_selected,
            "appraisal": appraisal_selected,
            "internal_failure": failure_internal_cols,
            "external_failure": failure_external_cols,
        }

        failure_auto_col = None

        # ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš©ì´ ê°ê° ì„ íƒëœ ê²½ìš° ìë™ í•©ì‚° ì»¬ëŸ¼ ìƒì„±
        failure_source_cols = [
            c for c in (failure_internal_cols + failure_external_cols)
            if c in numeric_cols
        ]

        # í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì´ ë”°ë¡œ ì„ íƒë˜ì§€ ì•Šì•˜ê³ , ë‚´ë¶€/ì™¸ë¶€ í•©ì‚°ì´ ê°€ëŠ¥í•˜ë©´ ìë™ ìƒì„±
        if failure_single_col == "(ì—†ìŒ)" and failure_source_cols:
            failure_auto_col = "FAILURE_COST_AUTO"
            # ì´ë¯¸ ê°™ì€ ì´ë¦„ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ì§€ ì•Šê³  ì´ë¦„ ë³€ê²½
            suffix = 1
            while failure_auto_col in df.columns:
                failure_auto_col = f"FAILURE_COST_AUTO_{suffix}"
                suffix += 1

            df[failure_auto_col] = df[failure_source_cols].sum(axis=1)

            st.info(
                f"ë‚´ë¶€/ì™¸ë¶€ ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ {failure_source_cols} ë¥¼ í•©ì‚°í•˜ì—¬ "
                f"'{failure_auto_col}' ì¹¼ëŸ¼ì„ ìë™ ìƒì„±í–ˆìŠµë‹ˆë‹¤. "
                f"íƒ€ê¹ƒ ì»¬ëŸ¼ ì„ íƒì—ì„œ ì´ ì¹¼ëŸ¼ì„ ì„ íƒí•˜ë©´ 'ì‹¤íŒ¨ë¹„ìš©' íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        elif failure_single_col != "(ì—†ìŒ)":
            # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ í†µí•© ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì„ ì§€ì •í•œ ê²½ìš°
            failure_auto_col = failure_single_col

        # -------------------------
        # íƒ€ê¹ƒ ìë™ íƒì§€ ë° ëª¨ë¸ë§
        # -------------------------
        st.write("### íƒ€ê¹ƒ(ì„±ê³µ/ì‹¤íŒ¨ ë˜ëŠ” í’ˆì§ˆ ê²°ê³¼) ì»¬ëŸ¼ ì„ íƒ")
        # ì‹¤íŒ¨ë¹„ìš© ë¶„ì„ ì¤‘ì‹¬ì´ë¯€ë¡œ, ìë™ ìƒì„±/ì§€ì •ëœ ì‹¤íŒ¨ë¹„ìš© ì¹¼ëŸ¼ì´ ìˆìœ¼ë©´ ì´ë¥¼ íƒ€ê¹ƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        default_target = None
        if failure_auto_col is not None and failure_auto_col in df.columns:
            default_target = failure_auto_col
        else:
            default_target = detect_target_column(df)

        target_col = st.selectbox(
            "íƒ€ê¹ƒ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš” (ì‹¤íŒ¨ë¹„ìš© ë˜ëŠ” í’ˆì§ˆ ê²°ê³¼ ì»¬ëŸ¼)",
            options=["(ì‚¬ìš© ì•ˆ í•¨)"] + list(df.columns),
            index=1 + (list(df.columns).index(default_target) if default_target in df.columns else 0)
        )


        # ìˆ«ì ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ë‹¨ìˆœí™”)
        numeric_df = df.select_dtypes(include=[np.number]).copy()

        if target_col != "(ì‚¬ìš© ì•ˆ í•¨)" and target_col in df.columns:
            # íƒ€ê¹ƒ ì²˜ë¦¬
            target_series = binarize_target(df[target_col])

            # íƒ€ê¹ƒì´ 0/1ì¸ì§€, ì—°ì†í˜•ì¸ì§€ í™•ì¸
            unique_vals = target_series.dropna().unique()
            is_binary = len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1})

            # numeric_dfì— íƒ€ê¹ƒ ë¶™ì´ê¸°
            numeric_df[target_col] = target_series

            # ê²°ì¸¡ì¹˜ ì œê±°
            data = numeric_df.dropna(subset=[target_col])
            if data.shape[0] < 4:
                st.warning("ìœ íš¨í•œ ë°ì´í„° í–‰ì´ 4ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.")
            else:
                X = data.drop(columns=[target_col])
                y = data[target_col]
                
                # ---------------------------
                # ğŸ”¥ ì‹¤íŒ¨ë¹„ìš© ê´€ë ¨ ì»¬ëŸ¼ ì œê±° (ì¤‘ìš”)
                # ---------------------------
                failure_related_cols = failure_internal_cols + failure_external_cols

                # ìë™ ìƒì„±ëœ í†µí•© ì‹¤íŒ¨ë¹„ìš©
                if failure_auto_col is not None:
                    failure_related_cols.append(failure_auto_col)

                # ì‚¬ìš©ìê°€ ì§ì ‘ ì„ íƒí•œ í†µí•© ì‹¤íŒ¨ë¹„ìš©
                if failure_single_col != "(ì—†ìŒ)":
                    failure_related_cols.append(failure_single_col)

                X = remove_failure_related_features(X, failure_related_cols)
                

                st.write("### ëª¨ë¸ í•™ìŠµ ê²°ê³¼")

                if is_binary:
                    st.write("#### (ë¶„ë¥˜) ì„±ê³µ/ì‹¤íŒ¨ ë˜ëŠ” ì–‘í’ˆ/ë¶ˆëŸ‰ ì˜ˆì¸¡")

                    clf_results, (X_train, X_test, y_train, y_test) = train_models_classification(X, y)

                    # ê²°ê³¼ í…Œì´ë¸”
                    rows = []
                    for name, r in clf_results.items():
                        rows.append({
                            "Model": name,
                            "Accuracy": r["accuracy"],
                            "F1-score": r["f1"],
                            "ROC-AUC": r["auc"],
                        })
                    st.dataframe(pd.DataFrame(rows).set_index("Model"))

                    # ëœë¤í¬ë ˆìŠ¤íŠ¸ ì¤‘ìš”ë„ + SHAP: í…Œì´ë¸” í˜•íƒœ
                    if "RandomForest" in clf_results:
                        rf_model = clf_results["RandomForest"]["model"]
                        st.write("#### ëœë¤í¬ë ˆìŠ¤íŠ¸ ë³€ìˆ˜ ì¤‘ìš”ë„")
                        imp_df = get_feature_importance_df(rf_model, X.columns)
                        if imp_df is not None:
                            st.dataframe(imp_df)

                        st.write("#### ëœë¤í¬ë ˆìŠ¤íŠ¸ SHAP ì¤‘ìš”ë„")
                        shap_df = get_shap_importance_df(rf_model, X_train, X.columns)
                        st.dataframe(shap_df)

                    # XGBoostë„ ìˆ«ì í…Œì´ë¸”ë¡œë§Œ
                    if XGBOOST_AVAILABLE and "XGBoost" in clf_results:
                        xgb_model = clf_results["XGBoost"]["model"]
                        st.write("#### XGBoost ë³€ìˆ˜ ì¤‘ìš”ë„")
                        imp_df_xgb = get_feature_importance_df(xgb_model, X.columns)
                        if imp_df_xgb is not None:
                            st.dataframe(imp_df_xgb)

                        st.write("#### XGBoost SHAP ì¤‘ìš”ë„")
                        shap_df_xgb = get_shap_importance_df(xgb_model, X_train, X.columns)
                        st.dataframe(shap_df_xgb)

                    # ë¶„ì„ ìš”ì•½ ìƒì„± (ì±—ë´‡ìš©)
                    summary_lines = ["[ë¶„ë¥˜ ëª¨ë¸ ìš”ì•½]"]
                    for name, r in clf_results.items():
                        summary_lines.append(
                            f"- {name}: Accuracy={r['accuracy']:.3f}, F1={r['f1']:.3f}, AUC={r['auc']:.3f}"
                        )
                    summary_lines.append(f"- ì‚¬ìš©ëœ íŠ¹ì§• ìˆ˜: {X.shape[1]}")
                    summary_lines.append(f"- íƒ€ê¹ƒ ì»¬ëŸ¼: {target_col}")
                    st.session_state["analysis_summary"] = "\n".join(summary_lines)

                else:
                    st.write("#### (íšŒê·€) ì—°ì†í˜• í’ˆì§ˆ ì§€í‘œ ì˜ˆì¸¡")

                    reg_results, (X_train, X_test, y_train, y_test) = train_models_regression(X, y)

                    rows = []
                    for name, r in reg_results.items():
                        rows.append({
                            "Model": name,
                            "RMSE": r["rmse"],
                            "RÂ²": r["r2"],
                        })
                    st.dataframe(pd.DataFrame(rows).set_index("Model"))

                    
                    if "RandomForest" in reg_results:
                        rf_model = reg_results["RandomForest"]["model"]
                        st.write("#### ëœë¤í¬ë ˆìŠ¤íŠ¸ ë³€ìˆ˜ ì¤‘ìš”ë„")
                        imp_df = get_feature_importance_df(rf_model, X.columns)
                        if imp_df is not None:
                            st.dataframe(imp_df)

                        st.write("#### ëœë¤í¬ë ˆìŠ¤íŠ¸ SHAP ì¤‘ìš”ë„")
                        shap_df = get_shap_importance_df(rf_model, X_train, X.columns)
                        st.dataframe(shap_df)

                    if XGBOOST_AVAILABLE and "XGBoost" in reg_results:
                        xgb_model = reg_results["XGBoost"]["model"]
                        st.write("#### XGBoost ë³€ìˆ˜ ì¤‘ìš”ë„")
                        imp_df_xgb = get_feature_importance_df(xgb_model, X.columns)
                        if imp_df_xgb is not None:
                            st.dataframe(imp_df_xgb)

                        st.write("#### XGBoost SHAP ì¤‘ìš”ë„")
                        shap_df_xgb = get_shap_importance_df(xgb_model, X_train, X.columns)
                        st.dataframe(shap_df_xgb)

                    summary_lines = ["[íšŒê·€ ëª¨ë¸ ìš”ì•½]"]
                    for name, r in reg_results.items():
                        summary_lines.append(
                            f"- {name}: RMSE={r['rmse']:.3f}, RÂ²={r['r2']:.3f}"
                        )
                    summary_lines.append(f"- ì‚¬ìš©ëœ íŠ¹ì§• ìˆ˜: {X.shape[1]}")
                    summary_lines.append(f"- íƒ€ê¹ƒ ì»¬ëŸ¼: {target_col}")
                    st.session_state["analysis_summary"] = "\n".join(summary_lines)

        else:
            st.info("íƒ€ê¹ƒ ì»¬ëŸ¼ì„ '(ì‚¬ìš© ì•ˆ í•¨)'ì´ ì•„ë‹Œ ì‹¤ì œ í’ˆì§ˆ ê²°ê³¼ ì»¬ëŸ¼ìœ¼ë¡œ ì„ íƒí•˜ë©´ ì˜ˆì¸¡ ëª¨ë¸ì´ í•™ìŠµë©ë‹ˆë‹¤.")


        # ì‹¤íŒ¨ë¹„ìš© ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‚¬ìš©í•  íƒ€ê¹ƒ ì»¬ëŸ¼ ê²°ì •
        failure_target_col = None
        # 1) ìë™ ìƒì„±/ì§€ì •ëœ ì‹¤íŒ¨ë¹„ìš© ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if failure_auto_col is not None and failure_auto_col in df.select_dtypes(include=[np.number]).columns:
            failure_target_col = failure_auto_col
        # 2) ê·¸ë ‡ì§€ ì•Šê³ , í˜„ì¬ íƒ€ê¹ƒ ì»¬ëŸ¼ì´ ìˆ«ìí˜•ì´ë©´ ê·¸ íƒ€ê¹ƒì„ ì‹¤íŒ¨ë¹„ìš©ìœ¼ë¡œ ê°€ì •
        elif target_col != "(ì‚¬ìš© ì•ˆ í•¨)" and target_col in df.select_dtypes(include=[np.number]).columns:
            failure_target_col = target_col

        st.markdown("---")
        st.write("### ì˜ˆë°©/í‰ê°€ë¹„ìš©ì„ ëŠ˜ë ¸ì„ ë•Œ ì‹¤íŒ¨ë¹„ìš© ì‹œë‚˜ë¦¬ì˜¤")

        scenario_info, err = build_scenario_result(df, cost_cols, failure_col_name=failure_target_col)

        
        if err:
            st.warning(err)
        else:
            base = scenario_info["baseline_cost"]

            colA, colB = st.columns(2)
            with colA:
                prevent_pct = st.slider("ì˜ˆë°©ë¹„ìš© ì¦ê°€ìœ¨ (%)", -50, 200, 0, step=1)
            with colB:
                appraisal_pct = st.slider("í‰ê°€/ê²€ì‚¬ë¹„ìš© ì¦ê°€ìœ¨ (%)", -50, 200, 0, step=1)

            new_cost = scenario_info["predict_func"](
                prevent_pct / 100.0,
                appraisal_pct / 100.0
            )

            diff = new_cost - base
            ratio = (new_cost / base - 1) * 100 if base != 0 else 0

            st.write(f"- ê¸°ì¤€ ì˜ˆìƒ ì‹¤íŒ¨ë¹„ìš©(í‰ê·  ê¸°ì¤€): **{base:,.2f}**")
            st.write(f"- ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆìƒ ì‹¤íŒ¨ë¹„ìš©: **{new_cost:,.2f}**")
            st.write(f"- ë³€í™”ëŸ‰: **{diff:,.2f} ({ratio:+.1f}%)**")

            if diff < 0:
                st.success("ì´ ì‹œë‚˜ë¦¬ì˜¤ ì¼ ë•Œ, ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ì‹¤íŒ¨ë¹„ìš©ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ë³´ì…ë‹ˆë‹¤.")
            elif diff > 0:
                st.warning("ì´ ë°ì´í„°ì—ì„œëŠ” ì˜ˆë°©/í‰ê°€ë¹„ìš© ì¦ê°€ê°€ ì˜¤íˆë ¤ ì‹¤íŒ¨ë¹„ìš© ì¦ê°€ì™€ í•¨ê»˜ ë‚˜íƒ€ë‚  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ê³µì • êµ¬ì¡°ë¥¼ ë‹¤ì‹œ ì ê²€í•´ ë³´ì„¸ìš”.")
            else:
                st.info("í˜„ì¬ ì„¤ì •ì—ì„œëŠ” ì‹¤íŒ¨ë¹„ìš© ë³€í™”ê°€ ê±°ì˜ ì—†ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")

            # ì´ ë¶€ë¶„ë„ ì±—ë´‡ ë¶„ì„ ìš”ì•½ì— ì¶”ê°€
            extra = f"\n\n[ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„]\n- ê¸°ì¤€ ì‹¤íŒ¨ë¹„ìš©: {base:,.2f}\n- ì‹œë‚˜ë¦¬ì˜¤ ì‹¤íŒ¨ë¹„ìš©: {new_cost:,.2f}\n- ë³€í™”ìœ¨: {ratio:+.1f}%"
            st.session_state["analysis_summary"] += extra

    else:
        st.info("CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")


with tab_chat:
    st.subheader("Q-COST AI ì»¨ì„¤í„´íŠ¸ì™€ ëŒ€í™”í•˜ê¸°")
    st.caption("**ì˜ˆì‹œ) ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ê²°ê³¼ í•´ì„í•´ì¤˜.**")
    st.caption("**ì˜ˆì‹œ) ëª¨ë¸ ì„±ëŠ¥ì„ ìš”ì•½í•´ì¤˜.**")

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("Q-COST, í’ˆì§ˆë¹„ìš©, ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.")
    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state["messages"].append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            if not google_api_key:
                st.warning("Google API KEYë¥¼ ì‚¬ì´ë“œë°”ì— ë¨¼ì € ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            else:
                # ìµœì‹  ë¶„ì„ ìš”ì•½ ë„˜ê²¨ì„œ ë‹µë³€
                analysis_summary = st.session_state.get("analysis_summary", "")
                try:
                    answer = generate_ai_response(
                        user_input,
                        api_key=google_api_key,
                        analysis_summary=analysis_summary
                    )
                except Exception as e:
                    answer = f"Gemini í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n\n{e}"

                st.markdown(answer)
                st.session_state["messages"].append({"role": "assistant", "content": answer})
